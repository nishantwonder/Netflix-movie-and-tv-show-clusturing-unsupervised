{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - NETFLIX MOVIES AND TV SHOWS CLUSTERING\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Nishant kumar singh"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an Unsupervised Machine learning project in this project I will have to build a model that can be capable of clustering different different types of data. The dataset is about netflix shows which has 7787 rows and 12 columns like show_id which represents ID of the show, type represents type of the show, title represents show title, cast represents name of the casting stars, country represents the country of the show, date added represents the date when the show is added to netflix, release_year represents the year the show was released,rating represents the rating of the show, duration represents the length of the show, listed_in tells what type and where the show belongs from, description gives short descriptions about the show. My task is to read and understand the data after that I will have to show some meaningfull charts and explain everything about the chart then according to the visualization chart I will have to make some hypothesis assumptions about the project then testing the assumptions. Then I will handle missing and null values and outliers after completing these all task I will look for imbalanced data if there is any imbalanced data then I will have to deal with that. Then I will select some important features further I will split the data for test and training purpose."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/nishantwonder/Netflix-movie-and-tv-show-clusturing-unsupervised.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Task is to make a Model that can cluster similar type of content together."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "from numpy import math\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from google.colab import drive\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.decomposition import PCA\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import silhouette_samples,silhouette_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df=pd.read_csv(\"/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "df.isnull().sum().plot.bar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset there are 7787 row and 12 column. Director column has highest number of missing value."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has total 7787 rows and 12 columns:\n",
        "\n",
        "1.Show_id : show ID\n",
        "\n",
        "2.type : Type of the show like movie or TV show\n",
        "\n",
        "3.title : Title of the show\n",
        "\n",
        "4.director : Director of the show\n",
        "\n",
        "5.cast : Actors and actress of the show\n",
        "\n",
        "6.country : Country of the origin of the show\n",
        "\n",
        "7.date_added : Date when the show added to netflix\n",
        "\n",
        "8.release_year : Release year\n",
        "\n",
        "9.rating : rating\n",
        "\n",
        "10.duration : duration of the show\n",
        "\n",
        "11.listed_in : Listed in\n",
        "\n",
        "12.description : Description of the show"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values for each variable\n",
        "for col in df.columns:\n",
        "  print('Unique values for ',col,'are \\n',df[col].unique(),'\\n')"
      ],
      "metadata": {
        "id": "yqLCdDkdtNY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready\n",
        "# Netflix Wrangling Class for data wrangling\n",
        "class NetflixWrangling:\n",
        "    # Init function\n",
        "    def __init__(self,df):\n",
        "      ''' init method '''\n",
        "      try:\n",
        "          self.df = df.copy()\n",
        "          self.df['duration_value'] = self.df['duration'].apply(self.set_duration_value)\n",
        "          self.df['duration_unit'] = self.df['duration'].apply(self.set_duration_unit)\n",
        "          self.cast_ = self.get_cast_or_listed_in('cast')\n",
        "          self.genres_ = self.get_cast_or_listed_in('listed_in')\n",
        "          self.directors_ = self.getTopValues('director')\n",
        "          self.countries_ = self.getTopValues('country')\n",
        "          self.release_years_ = self.getTopValues('release_year')\n",
        "          self.ratings_ = self.getTopValues('rating')\n",
        "          self.df['duration_value'] = self.df['duration_value'].apply(lambda x:int(x))\n",
        "            \n",
        "      except:\n",
        "          print('Invalid input!')\n",
        "    # Function to get dataset\n",
        "    def get_df(self):\n",
        "      ''' Function returns dataframe '''\n",
        "      return self.df\n",
        "    # Function to get movies dataset\n",
        "    def getMovies(self):\n",
        "      ''' Function returns returns dataframe which contains data of movies '''\n",
        "      return self.df[self.df['type'] == 'Movie']\n",
        "    # Function to get TV Show's dataset\n",
        "    def getTvShows(self):\n",
        "      ''' Function returns dataframe which contains data of tv shows '''\n",
        "      return self.df[self.df['type'] == 'TV Show']\n",
        "    # Function to get top values\n",
        "    def getTopValues(self,col):\n",
        "      ''' Function returns top values for the given column '''\n",
        "      try:\n",
        "          return self.df[col].value_counts().sort_values(ascending=False)\n",
        "      except:\n",
        "          return 'You have entered invalid column!'\n",
        "    # Function to set duration value\n",
        "    def set_duration_value(self,string):\n",
        "      ''' Function for setting duration values '''\n",
        "      if string[-7:] == 'Seasons':\n",
        "        return string[:-8]\n",
        "      elif string[-6:] == 'Season':\n",
        "        return string[:-7]\n",
        "      else:\n",
        "        return string[:-4]\n",
        "    \n",
        "    # Function to set duration unit\n",
        "    def set_duration_unit(self,duration):\n",
        "      ''' function for setting duration unit. '''\n",
        "      if (duration[-7:] == 'Seasons') | (duration[-6:] == 'Season'):\n",
        "        return 'season'\n",
        "      else:\n",
        "        return 'min'\n",
        "    \n",
        "    # Function to get cast or genres\n",
        "    def get_cast_or_listed_in(self,col):   \n",
        "        df = self.df.copy()\n",
        "        # internal function 1\n",
        "        def __get_list():\n",
        "            '''\n",
        "            takes dataset and column's name, returns list.\n",
        "            '''\n",
        "            list_1=[]\n",
        "            for i in df[df[col].isna()== False][col]:\n",
        "                list_1.append(i.split(', '))\n",
        "            return list_1\n",
        "          # internal function 2\n",
        "        def __get_list_vars(list_1):\n",
        "            list_2 = []\n",
        "            for i in list_1:\n",
        "              for j in i:\n",
        "                list_2.append(j)\n",
        "            return list_2\n",
        "\n",
        "          # internal function 3\n",
        "        def __get_dict_vars(list_2):\n",
        "            unique_var = set(list_2)\n",
        "            dict_of_vars = dict()\n",
        "            for i in unique_var:\n",
        "              dict_of_vars[i] = list_2.count(i)\n",
        "            return dict_of_vars\n",
        "        # Assigning values in list 1 and list 2 then returning the series\n",
        "        list_1 = __get_list()\n",
        "        list_2 = __get_list_vars(list_1)        \n",
        "        return pd.Series(__get_dict_vars(list_2)).sort_values(ascending=False)\n",
        "\n",
        "    # Function for converting duration_value into int type\n",
        "    def convertInt(self,col):\n",
        "      ''' Function to convert other type into integer '''\n",
        "      return int(col)\n",
        "    \n",
        "    # Get showsPerMonth\n",
        "    def getShowsPerMonth(self,typ='all'):\n",
        "      ''' Function which returns Shows per month '''\n",
        "      df = pd.DataFrame()\n",
        "      if typ == 'tv':\n",
        "          df = self.getTvShows()\n",
        "      elif typ == 'movie':\n",
        "          df = self.getMovies()\n",
        "      else:\n",
        "          df = self.df.copy()\n",
        "      df = df[df['date_added'].isna() == False]\n",
        "      df['added_month'] = df['date_added'].apply(lambda x:x.split()[0])\n",
        "      monthly_shows = df['added_month'].value_counts()\n",
        "      return monthly_shows\n",
        "\n",
        "    # Get Text Length\n",
        "    def getTextLength(self,text):\n",
        "      return len(text)\n",
        "    # Get Dataframe with length column\n",
        "    def getDfWithLength(self,col):\n",
        "      df = self.df.copy()\n",
        "      df['text_length'] = df[col].apply(self.getTextLength)\n",
        "      return df\n",
        "    \n",
        "# Initializing the object\n",
        "netflixWrangling = NetflixWrangling(df)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"type\"].value_counts()"
      ],
      "metadata": {
        "id": "L_3SafJK0SZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top count of show from top ten country  \n",
        "df[\"country\"].value_counts().head(10)"
      ],
      "metadata": {
        "id": "FiB1QNwH0lYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the shapes for each type of shows\n",
        "print('Data for Movies has shape :',netflixWrangling.getMovies().shape)\n",
        "print('')\n",
        "print('Data for TV shows has shape :',netflixWrangling.getTvShows().shape)\n",
        "print('')"
      ],
      "metadata": {
        "id": "O8wz6af23xjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total length for each type of durations\n",
        "netflixWrangling.get_df().groupby(['duration_unit'])['duration_value'].sum()"
      ],
      "metadata": {
        "id": "0PHXR4gS37-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of Shows for each month\n",
        "print('Total number of Shows for each month')\n",
        "netflixWrangling.getShowsPerMonth()"
      ],
      "metadata": {
        "id": "qYwfcV7e4Gia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of Movies for each month\n",
        "print('Total number of Movies for each month')\n",
        "netflixWrangling.getShowsPerMonth('movie')"
      ],
      "metadata": {
        "id": "9VmWaQaH4bz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of TV Shows for each month\n",
        "print('Total number of TV Shows for each month')\n",
        "netflixWrangling.getShowsPerMonth('tv')"
      ],
      "metadata": {
        "id": "8um3vgjZ4iNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning top 5 actors and genres to a variable\n",
        "print('Top 5 actors with highest number of shows :')\n",
        "netflixWrangling.cast_.head()"
      ],
      "metadata": {
        "id": "nEoE6KgZ5Ikp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 directors with highest number of shows\n",
        "print('Top 5 directors with highest number of shows :')\n",
        "netflixWrangling.directors_.head()"
      ],
      "metadata": {
        "id": "EzbPiYRU5PZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 countries with highest number of shows\n",
        "print('Top 5 countries with highest number of shows :')\n",
        "netflixWrangling.countries_.head()"
      ],
      "metadata": {
        "id": "HBrjlGZM5SmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 years with highest number of shows released\n",
        "print('Top 5 years with highest number of shows released:')\n",
        "netflixWrangling.release_years_.head()"
      ],
      "metadata": {
        "id": "3piUoWeU5cH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top ratings with highest number of shows\n",
        "print('Top ratings with highest number of shows :')\n",
        "netflixWrangling.ratings_"
      ],
      "metadata": {
        "id": "pVtWS5UD5eaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 Genres with highest number of Movies/TV Shows\n",
        "print('Top 5 Genres with highest number of Movies/TV Shows')\n",
        "netflixWrangling.genres_.head()"
      ],
      "metadata": {
        "id": "6A540cVQ5meF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 directors with highest length of descriptions\n",
        "print('Top 10 Directors with highest length of descriptions')\n",
        "netflixWrangling.getDfWithLength('description').groupby('director')['text_length'].mean().sort_values(ascending=False).reset_index().head(10)"
      ],
      "metadata": {
        "id": "ach5eWOD5u5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First of I assigned all movies' data to a variable and TV shows' data to another variable and made a class which is responsible of extracting data from string and also made 2 functions for getting duration values and units.\n",
        "\n",
        "\n",
        "2. Directors with highest number of movies/tv shows are Raúl Campos and Jan Suter : 18\n",
        "\n",
        "3. Top 5 countries produced highest number of movies are :\n",
        "\n",
        "United States : 2555\n",
        "India : 923\n",
        "United Kingdom : 397\n",
        "Japan : 226\n",
        "South Korea : 183\n",
        "\n",
        "\n",
        "4. Highest movies produced in year 2018 : 1121\n",
        "\n",
        "5. Rating for highest number of movie is TV-MA : 2863\n",
        "\n",
        "6. Top 5 actors with highest number of movies are :\n",
        "\n",
        "\n",
        "Anupam Kher : 42\n",
        "Shah Rukh Khan : 35\n",
        "Naseeruddin Shah : 30\n",
        "Om Puri : 30\n",
        "Akshay Kumar : 29\n",
        "\n",
        "\n",
        "7. Top genres with highest number of movies are :\n",
        "\n",
        "International Movies : 2437\n",
        "Dramas : 2106\n",
        "\n",
        "\n",
        "8. Total Seasons : 4280 and Total Minutes : 533979.\n",
        "\n",
        "9. Number of TV Shows/Movies for each month\n",
        "\n",
        "December : 833\n",
        "\n",
        "October : 785\n",
        "\n",
        "January : 757\n",
        "\n",
        "November : 738\n",
        "\n",
        "March : 669\n",
        "\n",
        "September : 619\n",
        "\n",
        "August : 618\n",
        "\n",
        "April : 601\n",
        "\n",
        "July : 600\n",
        "\n",
        "May : 543\n",
        "\n",
        "June : 542\n",
        "\n",
        "February : 472"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for visualization\n",
        "class NetflixVisualization(NetflixWrangling):\n",
        "    bar_colors = ['violet','indigo','blue','g','yellow','orange','r']\n",
        "    # Init method\n",
        "    def __init__(self,df):\n",
        "      ''' Init method '''\n",
        "      self.df = df\n",
        "      self.df_tv = df[df['type'] == 'Tv_shows']\n",
        "\n",
        "    # Bar chart for value counts\n",
        "    def countBar(self,col,typ='default',top=10):\n",
        "      ''' Function to display top most values for a given column'''\n",
        "      try:\n",
        "        if typ == 'tv':\n",
        "          df = self.getTvShows()\n",
        "        elif typ == 'movie':\n",
        "          df = self.getMovies()\n",
        "        else:\n",
        "          df = self.df.copy()\n",
        "        df[col].value_counts().sort_values(ascending=False)[:top].plot.bar(color=self.bar_colors,figsize=(15,5))\n",
        "        plt.title('Number of shows for top '+str(top)+\" \"+col)\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('counts')\n",
        "        plt.show()\n",
        "      except:\n",
        "        print('Invalid Input...')\n",
        "\n",
        "    # Barh chart for value counts\n",
        "    def countBarh(self,col,typ='default',top=10):\n",
        "      ''' Function to display top most values for a given column '''\n",
        "      try:\n",
        "        if typ == 'tv':\n",
        "          df = self.getTvShows()\n",
        "        elif typ == 'movie':\n",
        "          df = self.getMovies()\n",
        "        else:\n",
        "          df = self.df.copy()\n",
        "        df[col].value_counts().sort_values(ascending=False)[:top].plot.barh(color=self.bar_colors,figsize=(13,5))\n",
        "        plt.title('Number of shows for top '+str(top)+\" \"+col)\n",
        "        plt.xlabel('counts')\n",
        "        plt.ylabel(col)\n",
        "        plt.show()\n",
        "      except:\n",
        "        print('Invalid Input...')\n",
        "    \n",
        "    # Chart 1 - Function shows contents counts for each type\n",
        "    def chart_1(self):\n",
        "      ''' Function shows contents counts for each type  '''\n",
        "      self.df['type'].value_counts().plot.pie(figsize=(7,7),\n",
        "                                                  colors={'cyan','coral'},\n",
        "                                                  shadow=True,explode=[0.03,0.03],\n",
        "                                                  autopct='%0.01f%%')\n",
        "      plt.show()\n",
        "        \n",
        "    # Chart 2 - Bar chart shows top countries with highest number of shows\n",
        "    def chart_2(self,typ='default',top=10):\n",
        "      ''' Function shows top countries with highest number of shows '''\n",
        "      self.countBar('country',typ,top)\n",
        "        \n",
        "        \n",
        "    # Chart 3 - Bar chart,directors for TV shows\n",
        "    def chart_3(self,typ='default',top=10):\n",
        "      ''' Function shows top directors with highest number of shows '''\n",
        "      self.countBar('director',typ,top)\n",
        "\n",
        "        \n",
        "    # chart 4 - Function shows top release with highest number of shows\n",
        "    def chart_4(self,typ='default',top=10):\n",
        "      ''' Function shows top release year with highest number of shows '''\n",
        "      self.countBar('release_year',typ,top)\n",
        "\n",
        "     # Chart 5 - shows top ratings with highest number of shows\n",
        "    def chart_5(self,typ='default',top=10):\n",
        "      ''' Function shows top ratings with highest number of shows '''\n",
        "      if typ == 'movie':\n",
        "        plt.xticks(np.arange(0,1851,100))\n",
        "        self.countBarh('rating',typ,top)\n",
        "      else:\n",
        "        plt.yticks(np.arange(0,1201,100))\n",
        "        self.countBar('rating',typ,top)\n",
        "\n",
        "        \n",
        "    # Chart 6- shows top listed in with highest number of shows\n",
        "    def chart_6(self,typ='default',top=10):\n",
        "      ''' Function shows top listed_in with highest number of shows '''\n",
        "      plt.xticks(np.arange(0,400,10))\n",
        "      self.countBarh('listed_in',typ,top)\n",
        "    # Method to get top  values for the given column\n",
        "    def getTopBarh(self,col):\n",
        "      ''' Function shows top 5 values for a given column '''\n",
        "      x = self.get_cast_or_listed_in(col).head()\n",
        "      x.plot.barh(color= self.bar_colors)\n",
        "      plt.title('Top 5 '+col)\n",
        "      plt.xlabel('count of movies/tv shows')\n",
        "      plt.ylabel(col)\n",
        "      plt.show()\n",
        "        \n",
        "# Initializing the object\n",
        "netflixVisualization = NetflixVisualization(df)\n"
      ],
      "metadata": {
        "id": "AAoJKTNw7SYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# value counts for each type\n",
        "netflixVisualization.chart_1()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie chart expresses part to whole relationship with the data so I picked this chart to see percentage of TV shows and movies the in dataset."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found that there are 69.1% movies and 30.9% TV Shows in the dataset."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, percentage of movies are more the double of TV Shows. In movie industries competition is more than double."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#Top 10 countries with highest number of shows\n",
        "netflixVisualization.chart_2()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart show value counts or frequency of the data for different columns and I picked this chart to see which are top 10 countries producing highest number of movies/tv shows."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found that United State is producing highest number of movies that is more than 2500 movies then India is 2nd highest about 1000 after that United Kingdom producting about 500 movies/TV shows."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, most of the movies are producing in USA and India they can sell good movies in competitive price to netflix."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Top 10 directors with highest number of TV shows\n",
        "netflixVisualization.chart_3('tv')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 directors with highest number of Movies\n",
        "netflixVisualization.chart_3('movie')"
      ],
      "metadata": {
        "id": "kVJmfL1586IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart shows value counts and frequency of the data for each columns and I picked this chart to see top directors with highest number of TV shows and Movies."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 10 Directors for TV Shows.\n",
        " Alastair Fothergill : 3 TV shows.\n",
        "\n",
        "Ken Burns : 2 TV Shows.\n",
        "\n",
        "Shin Won-ho : 2 TV Shows.\n",
        "\n",
        "Iginio Straffi : 2 TV Shows.\n",
        "\n",
        "Rob Seidenglanz : 2 TV Shows.\n",
        "\n",
        "Stan Lathan : 2 TV Shows.\n",
        "\n",
        "Rest of them produced only 1 TV show.\n",
        "\n",
        "Top 10 Directors for Movies.\n",
        "\n",
        "Raúl Campos and Jan Suter : 18 movies.\n",
        "\n",
        "Marcus Raboy : 15 movies.\n",
        "\n",
        "Jay Karas : 14 movies.\n",
        "\n",
        "Cathy Garcia-Molina : 13 movies.\n",
        "\n",
        "Youssef Chahine : 12 movies.\n",
        "\n",
        "Jay Chapman : 12 Movies.\n",
        "\n",
        "Martin Scorsese : 12 Movies.\n",
        "\n",
        "Steven Spielberg : 10 Movies.\n",
        "\n",
        "David Dhawan : 9 Movies.\n",
        "\n",
        "Johnnie To : 8\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum 3 TV Shows are produced by 1 director in other side maximum number of movies are 18 that is 6 times greater than TV shows. Movies are more in demand than TV shows becacuse most of the movies shows the ending part quickly and on other hand it takes much time to finish the TV shows completly."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "#Top 5 release year with highest number of Movies\n",
        "netflixVisualization.chart_4('movie',top=5)"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Top 5 release year with highest number of TV shows\n",
        "netflixVisualization.chart_4('tv',top=5)"
      ],
      "metadata": {
        "id": "NBY07Rxy9u50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart shows value counts and frequency of the data for each columns and I picked this chart to see nomber of shows for each year."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Movies released in each year:\n",
        "\n",
        "2017 : 744\n",
        "\n",
        "2018 : 734\n",
        "\n",
        "2016 : 642\n",
        "\n",
        "2019 : 582\n",
        "\n",
        "2020 : 411\n",
        "\n",
        "Number of TV Shows released in each year:\n",
        "\n",
        "2020 : 457\n",
        "\n",
        "2019 : 414\n",
        "\n",
        "2018 : 387\n",
        "\n",
        "2017 : 268\n",
        "\n",
        "2016 : 240"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highest number of movies released in the year 2017 and TV shows in 2020. The reason behind this is that TV shows are relatively small and movies takes much time when shooting. Since they shoots TV shows easily in advance therefore the number of TV Shows are higher in 2020 and movies are less because of corona virus."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#Top 5 rating with highest number of Movies\n",
        "netflixVisualization.chart_5('movie',top=5)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 rating with highest number of TV Shows\n",
        "netflixVisualization.chart_5('tv',top=5)"
      ],
      "metadata": {
        "id": "lUHjruGY-hZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart shows value counts and frequency of the data for each columns and I picked this chart to see number of shows for each catagory of ratings."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Movies for top 5 ratings:\n",
        "\n",
        "TV-MA : 1845\n",
        "\n",
        "TV-14 : 1272\n",
        "\n",
        "R : 663\n",
        "\n",
        "TV-PG : 505\n",
        "\n",
        "PG-13 : 386\n",
        "\n",
        "Numbere of TV Shows for top 5 ratings:\n",
        "\n",
        "TV-MA : 1018\n",
        "\n",
        "TV-14 : 659\n",
        "\n",
        "TV-PG : 301\n",
        "\n",
        "TV-Y7 : 176\n",
        "\n",
        "TV-Y : 163"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, The gained insight will help while making prediction."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#Top 5 genres with highest number of shows\n",
        "plt.xticks(np.arange(0,2601,200))\n",
        "netflixVisualization.getTopBarh('listed_in')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Top 5 Listed_in with highest number of Movies\n",
        "netflixVisualization.chart_6('movie',top=5)"
      ],
      "metadata": {
        "id": "_NhTOUBSJSa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 Listed_in with highest number of TV shows\n",
        "netflixVisualization.chart_6('tv',top=5)"
      ],
      "metadata": {
        "id": "qA3LjJpzJhI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart shows value counts and frequency of the data for each columns and I picked this chart to see number of shows for each catagory of genres."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 10 genres with highest number of movies/tv shows.\n",
        "\n",
        "\n",
        "International Movies : 2437\n",
        "\n",
        "Dramas : 2106\n",
        "\n",
        "Comedies : 1471\n",
        "\n",
        "International TV Shows : 1199\n",
        "\n",
        "Documentaries : 786\n",
        "\n",
        "Genres with highest number of movies :\n",
        "\n",
        "Documentaries : 334\n",
        "\n",
        "Stand-Up Comedy : 321\n",
        "\n",
        "Dramas, International Movies : 320\n",
        "\n",
        "Comedies, Dramas, International Movies : 243\n",
        "\n",
        "Dramas, Independent Movies, International Movies : 215\n",
        "\n",
        "Genres with highest number of TV Shows:\n",
        "\n",
        "Kids' TV : 205\n",
        "\n",
        "International TV Shows, TV Dramas : 111\n",
        "\n",
        "Crime TV Shows, International TV Shows, TV Dramas : 106\n",
        "\n",
        "Kids' TV, TV Comedies : 90\n",
        "\n",
        "International TV Shows, Romantic TV Shows, TV Dramas : 86"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gained insight will help while making the prediction.\n",
        "\n",
        "Documentaries, Stand-up comedies, dramas, international movies, comedies and independent the genres which highest number of movies are being produced.\n",
        "Therefore these we can say that consumers mostly likes this kind of movies. 2. Kids TV, International TV shows, TV dramas, Crime tv shows, Romantic TV shows are the genres which are being produced in highest quantity."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "#Top 5 Actors with highest number of shows\n",
        "plt.xticks(np.arange(0,43,1))\n",
        "netflixVisualization.getTopBarh('cast')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart shows value counts and frequency of the data for each columns and I picked this chart to see top 10 actors with highest number of movies/tv show."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found that Anupam Kher worked in about 42 movies which is highest then Shahrukh Khan worked in about 37 movies that is 2nd highest, OM Puri, Nasisuddin Shah, Takahiro Sakurai, Akshay Kumar, Paresh Rawal, Yuki Kaji, Boman Irani and Amitabh Bachchan are from 3rd highest to 10 respectively."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gained insight will help while making the prediction. These are the top actors consumers likes to watch them and their movies mostly break the records and earns high profits."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "netflixWrangling.getShowsPerMonth().plot.barh(figsize=(12,4),color=netflixVisualization.bar_colors)\n",
        "plt.title('Monthly Show count')\n",
        "plt.xlabel('Counts')\n",
        "plt.ylabel('Months')\n",
        "plt.xticks(np.arange(0,851,50))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart shows value counts and frequency of the data for each column therefore I picked this chart to see counts of shows for each month."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found that october, november, december and january have more than 700 highest number of movies are produced but every month there are more than 400 movies or TV Shows are being produced."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the movies are get produced during winter season."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df_copy = df.copy()\n",
        "df_copy['cast'] = df_copy['cast'].fillna('unknown cast')\n",
        "df_copy['country'] = df_copy['country'].fillna('unknown country')\n",
        "df_copy.drop(columns=['director'],inplace=True)\n",
        "df_copy = df_copy.dropna()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all I replace nan with unknown cast and unknown country for columns cast and country. Then I dropped those rows which has less than or uqual to 10 nan values."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Function for displaying outliers\n",
        "def displayOutliers():\n",
        "  ''' This function display outliers '''\n",
        "  sns.boxplot(df_copy['release_year'])\n",
        "  plt.xticks(np.arange(1920,2031,10))\n",
        "  plt.show()\n",
        "\n",
        "# Function for outlier treatment\n",
        "def treatOutliers(df_copy):\n",
        "  ''' Function takes dataset and returns a dataset after treating the outliers '''\n",
        "  q1 = df_copy['release_year'].quantile(.25)\n",
        "  q3 = df_copy['release_year'].quantile(.75)\n",
        "  iqr = q3-q1\n",
        "  lower = q1-1.5*(iqr)\n",
        "  df_copy = df_copy[df_copy['release_year'] > lower]\n",
        "  return df_copy\n",
        "\n",
        "# Treating the outlier by applying the above function\n",
        "print('Before outlier treatment')\n",
        "displayOutliers()\n",
        "for i in range(2):\n",
        "    print('After iteration '+str(i+1))\n",
        "    df_copy=treatOutliers(df_copy)\n",
        "    displayOutliers()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used q1-1.5iqr technique to remove uni-variate outliers because the column release_year had outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df_copy['movie'] = df_copy['type'].apply(lambda x: 1 if x=='Movie' else 0)\n",
        "df_copy['tv_show'] = df_copy['type'].apply(lambda x: 0 if x=='Movie' else 1)\n",
        "df_copy.drop(columns=['type'],inplace=True)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot encoding technique is not biased to a particular variable therefore I did one hot encoding of column type and created new columns movie and tv_show.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the textual columns and assigning it to a new variable\n",
        "df_copy['cluster_col'] = (df_copy['cast']+' '+df_copy['listed_in']+' '+' '+df_copy['description'])"
      ],
      "metadata": {
        "id": "2B8eLEYaPbJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Function to convert in lower case\n",
        "def lower_casing(text):\n",
        "  ''' function takes string and conver it to lower case '''\n",
        "  list_of_words = [letter.lower() for letter in text.split()]\n",
        "  return ' '.join(list_of_words)\n",
        "  \n",
        "df_copy['cluster_col'] = df_copy['cluster_col'].apply(lower_casing)"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def rem_punctuation(text):\n",
        "  '''\n",
        "  takes text and removes punctuations\n",
        "  '''\n",
        "  return ''.join([t for t in text if t not in string.punctuation])\n",
        "  \n",
        "# Remove Punctuations\n",
        "df_copy['cluster_col'] = df_copy['cluster_col'].apply(lower_casing)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def removeDigits(text):\n",
        "  '''\n",
        "  takes text and removes digits\n",
        "  '''\n",
        "  nums = ['0','1','2','3','4','5','6','7','8','9']\n",
        "  return ''.join([t for t in text if t not in nums])\n",
        "\n",
        "df_copy['cluster_col'] = df_copy['cluster_col'].apply(removeDigits)\n",
        "\n",
        "# Function to remove url\n",
        "def removeUrls(text):\n",
        "  '''\n",
        "  Takes a texts and removes url\n",
        "  '''\n",
        "  return ' '.join([t for t in text.split('.') if t not in ['com','www','https//']])\n",
        "\n",
        "# Remove URLs & Remove words and digits contain digits\n",
        "df_copy['cluster_col'] = df_copy['cluster_col'].apply(removeUrls)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "# Function to remove stop words\n",
        "def removeStopWords(text):\n",
        "  '''\n",
        "  Takes text and removes stop words\n",
        "  '''\n",
        "  stopwords_ = nltk.corpus.stopwords.words('english')\n",
        "  return ' '.join([t for t in text.split() if t not in stopwords_])\n",
        "\n",
        "# Remove Stopwords\n",
        "print('Length before removing stopwords ',len(df_copy['cluster_col'][0]))\n",
        "df_copy['cluster_col'] = df_copy['cluster_col'].apply(removeStopWords)\n",
        "print('Length after removing stopwords ',len(df_copy['cluster_col'][0]))"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def removeWhiteSpace(text):\n",
        "  '''\n",
        "  takes text and removes white spaces\n",
        "  '''\n",
        "  return ' '.join([t for t in text.split(' ')])\n",
        "# Remove White spaces\n",
        "df_copy['cluster_col'] = df_copy['cluster_col'].apply(removeWhiteSpace)"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Vectorization Function\n",
        "def getVectorized(df):\n",
        "  ''' Function takes dataset and apply vectorization '''\n",
        "  count_vectorizer = CountVectorizer()\n",
        "  # Fitting the count vectorizer\n",
        "  count_vectorizer.fit_transform(df['cluster_col'])\n",
        "  # Collecting the vocabularies\n",
        "  return count_vectorizer.vocabulary_.items()\n",
        "\n",
        "# Applying the  above function and showing the features\n",
        "dictionary_cluster_col = getVectorized(df_copy)\n",
        "dictionary_cluster_col"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total features\n",
        "print('Total Features ',len(dictionary_cluster_col))"
      ],
      "metadata": {
        "id": "MxNjPNMAQpVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Stemming finction\n",
        "def stemming(text):\n",
        "  ''' Takes text and returns after stemming. '''\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  return ' '.join([stemmer.stem(t) for t in text.split()])\n",
        "\n",
        "# Function to assign words and counts from dictionary to list\n",
        "def assignWordsAndCountsToList(dictionary):\n",
        "  ''' Takes dictionary and return list of words and counts. '''\n",
        "  vocab_cluster = [ ]\n",
        "  vocab_counts_cluster = []\n",
        "  for key , value in dictionary:\n",
        "    vocab_cluster.append( key )\n",
        "    vocab_counts_cluster.append( value )\n",
        "  return vocab_cluster,vocab_counts_cluster\n",
        "\n",
        "# Function to display word count\n",
        "def displayTop20Words(df):\n",
        "  '''  Displays top 20 words. '''\n",
        "  plt.barh(df.head(20)['word'].values,df.head(20)['count'].values)\n",
        "  plt.xlabel('Count')\n",
        "  plt.ylabel('Words')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before stemming"
      ],
      "metadata": {
        "id": "51Yrek9_ES7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning words and counts to lists\n",
        "vocab_cluster,vocab_counts_cluster = assignWordsAndCountsToList(dictionary_cluster_col)\n",
        "# Words and counts before stemming\n",
        "df_cluster_before_stem = pd.DataFrame({'word':vocab_cluster,'count':vocab_counts_cluster}).sort_values(by='count',ascending=False)\n",
        "df_cluster_before_stem.head(20).T"
      ],
      "metadata": {
        "id": "vU8fFYL1DYvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting top 10 words\n",
        "plt.rcParams['figure.figsize'] = (15,7)\n",
        "plt.xlim(40225,40255)\n",
        "plt.title('Word counts before Stemming for Cluster_col')\n",
        "displayTop20Words(df_cluster_before_stem)"
      ],
      "metadata": {
        "id": "34wmm0cCDd6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming removes the extra tail of the words like coming converted to come. I am using this technique to remove the tail of the words."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After stemming"
      ],
      "metadata": {
        "id": "Mgkwn6FsEj6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming and assigning words to a variable\n",
        "df_copy['cluster_col'] = df_copy['cluster_col'].apply(stemming)\n",
        "dictionary_cluster_col = getVectorized(df_copy)\n",
        "# Assigning vocabularies and counts to the lists for listed_in.\n",
        "vocab_cluster , vocab_counts_cluster = assignWordsAndCountsToList(dictionary_cluster_col)\n",
        "# Words and counts after stemming\n",
        "df_cluster_after_stem = pd.DataFrame({'word':vocab_cluster,'count':vocab_counts_cluster}).sort_values(by='count',ascending=False)\n",
        "df_cluster_after_stem.head(20).T"
      ],
      "metadata": {
        "id": "SrqBGxkoEA86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting top 10 words\n",
        "plt.xlim(37230,37260)\n",
        "plt.title('Words count After Stemming for Cluster_col')\n",
        "displayTop20Words(df_cluster_after_stem)"
      ],
      "metadata": {
        "id": "k14PMhJhD3a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Function to get number from proportion\n",
        "def proportionToNumber(proportion,data):\n",
        "  return (proportion * data)/100\n",
        "\n",
        "num = int(round(proportionToNumber(1,len(df_copy)),0))\n",
        "# Vectorizing Text - discarding features which are available less then 1% of the dataset and more than 90%.\n",
        "tfidfvectorizer = TfidfVectorizer(min_df=num,max_df=.9)\n",
        "features_array = tfidfvectorizer.fit_transform(df_copy['cluster_col']).toarray()\n",
        "features_names = tfidfvectorizer.get_feature_names_out()\n",
        "df_copy.shape"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making dataframe with vocabularies\n",
        "X = pd.DataFrame(features_array,columns=features_names)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "ZLtStAaPE8RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used tfidfVectorization technique."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are a huge number of features and most of them are containing zeros therefore we need to do dimentionality reduction."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "# Dimensionality Reduction (If needed)\n",
        "pca = PCA(n_components=.975)\n",
        "pca.fit(X.values)\n",
        "X2 = pca.transform(X.values)\n",
        "len(X2[0])"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used principal component analysis."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display clusters\n",
        "def displayClusters(X,labels,centers):\n",
        "  ''' Takes data, labels and centers and plots the scatter chart '''\n",
        "  plt.scatter(X2[: , 0] , X2[: , 1] , c=labels,s=10,cmap='viridis')\n",
        "  plt.scatter(centers[:, 0],centers[:, 1],c='red',s=15)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "acTr0O2KI5cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation - KElbowVisualization\n",
        "plt.rcParams['figure.figsize'] = (12,7)\n",
        "model = KMeans(random_state=10)\n",
        "visualizer = KElbowVisualizer(model,k=(2,15),metric='calinski_harabasz',timings=False,locate_elbow=False)\n",
        "# Fit the Algorithm\n",
        "visualizer.fit(X2)\n",
        "# Predict on the model\n",
        "visualizer.show()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhouette Score for each cluster\n",
        "n_clusters_range = np.arange(2,15)\n",
        "for c in n_clusters_range:\n",
        "    clusterer = KMeans(n_clusters=c,random_state=10)\n",
        "    pred = clusterer.fit_predict(X2)\n",
        "    centers = clusterer.cluster_centers_\n",
        "    score = silhouette_score(X2,pred)\n",
        "    inertia = clusterer.inertia_\n",
        "    print('for cluster ',c,' the silhouette score is ',score)\n",
        "# plotting the results:\n",
        "    displayClusters(X2,pred,centers)"
      ],
      "metadata": {
        "id": "QCF1v4ZOLlGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhouette score is good at 6 clusters therefore I am taking n_clusters = 6.\n",
        "kmeans= KMeans(n_clusters=6, random_state=10,max_iter=100)\n",
        "kmeans.fit(X2)\n",
        "\n",
        "# predict the labels of clusters.\n",
        "labels = kmeans.fit_predict(X2)\n",
        "centers = kmeans.cluster_centers_\n",
        "# plotting the results:\n",
        "displayClusters(X2,labels,centers)"
      ],
      "metadata": {
        "id": "KOdtuOmAMkz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From KElbow visualization I found that after n_cluster = 6 the slop is very close to constant and silhouette score is also high that is .046"
      ],
      "metadata": {
        "id": "BPFTUz1_x0D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "mdl = KMeans()\n",
        "params ={'n_clusters':[6],'random_state':[10],'max_iter':[15,20],'tol':[.01,.1]}\n",
        "kmeans_gridcv = GridSearchCV(mdl,param_grid=params,verbose=2,cv=2)\n",
        "# Fit the Algorithm\n",
        "kmeans_gridcv.fit(X2)\n",
        "# Predict on the model\n",
        "best_kmeans = kmeans_gridcv.best_estimator_\n",
        "labels = best_kmeans.predict(X2)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_gridcv.best_params_"
      ],
      "metadata": {
        "id": "Cg75hdeiM6CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the results:\n",
        "centers = best_kmeans.cluster_centers_\n",
        "plt.scatter(X2[: , 0] , X2[: , 1] , c=labels,s=10,cmap='viridis')\n",
        "plt.scatter(centers[:, 0],centers[:, 1],c='red',s=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "05ixZgAHM81h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think cross validation is not needed for clustering.\n",
        "I used n_clusters and max_iter hyper parameters to tune the parameters and I did not found any improvement."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I did not see any improvement."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the dendogram to find the optimal number of clusters - and find largest vertical distance we can make without crossing any other horizontal line\n",
        "linkage =sch.linkage(X2, method = 'ward')\n",
        "dendrogram = sch.dendrogram(linkage)\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Shows')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "j9LgoUrGNaOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering to the mall dataset\n",
        "# Average and single linkage is not good\n",
        "hc = AgglomerativeClustering(n_clusters = 4, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X2)\n",
        "# Plotting the results:\n",
        "plt.scatter(X2[: , 0] , X2[: , 1] , c=y_hc,s=10,cmap='Paired')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F48MgIsMNd5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are lot of noice in the dataset therefore the result is not good but for n_clusters = 4 it seems better."
      ],
      "metadata": {
        "id": "Wk9vnqWwNtXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "dbscan = DBSCAN(eps=.91,min_samples=15,algorithm='auto')\n",
        "# Fit the Algorithm\n",
        "dbscan.fit(X2)\n",
        "# Predict on the model\n",
        "ypred = dbscan.fit_predict(X2)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(X2[:,0], X2[:,1], c=ypred,s=5,cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dbscan is not good if there are lot of noise in the dataset. Here we can see this algorithm is giving a big cluser and 3 to 4 very small clusters."
      ],
      "metadata": {
        "id": "-WJHTH_7SM4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Euclidean distance."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would choose Kmeans because the model is fast and giving better result than all the others."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has 7787 rows and 12 columns.\n",
        "\n",
        "There are 30.68% null values in director, 9.22% in cast column, 6.51% in \n",
        "country, 0.13 in date_added, and 0.09 on rating columns.\n",
        "\n",
        "The dataset containing only movies has shape (5377,14) and for TV Shows has (2410,14).\n",
        "\n",
        "Total number of features after vectorization is 40255.\n",
        "\n",
        "Total features after dimentionality reduction is 364.\n",
        "\n",
        "Kmeans clustering is giving good clusters therefore I would choose Kmeans with n_clusters = 6 because silhouette in this case is very good .046\n",
        "\n",
        "Agglomerative clustering is also good but for high number of clusters it is not giving better cluster.\n",
        "\n",
        "DBSCAN is giving a large size of cluster because my data is very noisy therefore in this case dbscan is not good."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}